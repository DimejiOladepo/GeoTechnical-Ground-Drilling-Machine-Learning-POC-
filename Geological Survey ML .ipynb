{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import time library and start timing the program\n",
    "import time\n",
    "start_time = time.time()\n",
    "#Import frameworks used by program\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn import metrics\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenNestedList(nestedList):\n",
    "    ''' Helper function which converts a nested list to a flat list '''\n",
    "    flatList = []\n",
    "    # Iterate over all the elements in given list\n",
    "    for elem in nestedList:\n",
    "        # Check if type of element is list\n",
    "        if isinstance(elem, list):\n",
    "            # Extend the flat list by adding contents of this element (list)\n",
    "            flatList.extend(flattenNestedList(elem))\n",
    "        else:\n",
    "            # Append the element to the list\n",
    "            flatList.append(elem)    \n",
    " \n",
    "    return flatList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapairlist(folder):\n",
    "    \n",
    "    \"\"\"This function creates a one to one list of lists containing pairs of \n",
    "Cone Penetration Test SND files and their corresponding Laboratory Test results in \n",
    "Workbook per sub-folder \n",
    "    \"\"\"\n",
    "    file_pair_list = list()\n",
    "    #Listing all subfolders in the GEOML-1 folder\n",
    "    geoML_subfolders = [ sub.path for sub in os.scandir(folder) if sub.is_dir() ]\n",
    "\n",
    "    for subfolder in geoML_subfolders:\n",
    "        #Creating list of directory path to laboratory report workbook files  \n",
    "        workbook_dir_list = list()\n",
    "        for root, dirs, files in os.walk(subfolder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xlsm\") or file.endswith(\".xlsx\"):\n",
    "                    workbook_dir_list.append(os.path.join(root,file))\n",
    "        #Extracting list of workbook id number \n",
    "        workbook_id = list()\n",
    "        for _id in workbook_dir_list:\n",
    "            workbook_id.append(_id[:-5].split()[-1])\n",
    "            \n",
    "        #Create dictionary of workbook id to workbook directory path\n",
    "        id_dictionary = dict(zip(workbook_id, workbook_dir_list))\n",
    "\n",
    "        #Create list of SND files in subfolders\n",
    "        snd_name_list = list()\n",
    "        for root, dirs, files in os.walk(subfolder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".SND\"):\n",
    "                    snd_name_list.append(file)\n",
    "        \n",
    "        #Match workbook id with SNDs \n",
    "        snd_match = list()\n",
    "        for element in snd_name_list:\n",
    "            if element[:-4] in workbook_id:\n",
    "                snd_match.append(os.path.join(root, element))\n",
    "\n",
    "        #Add matching Workbook to SNDs to list         \n",
    "        workbook_match = list()\n",
    "        for elements in snd_match:\n",
    "            identifier = elements[:-4].split(\"/\")\n",
    "            workbook_match.append(id_dictionary[identifier[-1]])\n",
    "\n",
    "        #Create list of list of folder lists containing Workbook-SND pairs matched by id \n",
    "        file_pairs = list(zip(workbook_match, snd_match))\n",
    "        file_pair_list.append(file_pairs)\n",
    "        \n",
    "    #Filter out empty lists\n",
    "    file_pair_list_filtered = list(filter(None, file_pair_list))\n",
    "    \n",
    "    #Creating list of list from file_pairs\n",
    "    pair_lst = list()\n",
    "    for level_2 in file_pair_list_filtered:\n",
    "        for level_1 in level_2:\n",
    "            pair_lst.append(list(level_1))\n",
    "    \n",
    "    return(pair_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary containing Soil type translation from Norwegian to English\n",
    "soil_type = {'LEIRE': 'CLAY', 'KVIKKLEIRE': 'QUICK CLAY', 'TØRRSKORPELEIRE': 'WEATHERED CLAY', \n",
    "             'SILT': 'SILT', 'TØRRSKORPESILT': 'WEATHERED SILT', 'SAND': 'SAND', \n",
    "             'GRUS': 'GRAVEL', 'TORV': 'PEAT', 'GYTJE': 'GYTJA', \n",
    "             'ORG. MATR.': 'ORG. MAT.', 'MATJORD': 'TOPSOIL', 'DY': 'DY',\n",
    "             'MATERIALE': 'MATERIAL', 'FYLLMASSE': 'FILL SOIL'}\n",
    "\n",
    "#List for incompatible pairs which are excluded\n",
    "incompatible_pairs = []\n",
    "\n",
    "#Creating empty dataframe with columns for housing extracted data\n",
    "big_merge_df = pd.DataFrame(columns = [\"Drill Depth (m)\", \"Soil type\", \"X-coordinate\", \"Y-coordinate\", \n",
    "                                       \"Height Above Sea Level\", \"Drill Pressure (kN)\",\"Flushing Pressure (kN)\"])\n",
    "\n",
    "for pair in datapairlist(\"GEOML-1\"):\n",
    "    try:\n",
    "        # read SNDs in datapairlist as comma separated values\n",
    "        snd_table = pd.read_csv(pair[1], delimiter = \"\\r\\n\", header = None, sep = \" \", names = \"v\")\n",
    "        snd_table_lst = []\n",
    "        # read values in 15th row and below\n",
    "        for row in snd_table[15:].values.tolist():\n",
    "            for column in row:\n",
    "                snd_table_lst.append(column.split())\n",
    "\n",
    "        # create dataframe from snd table list          \n",
    "        df = pd.DataFrame(snd_table_lst)\n",
    "        # use only four columns in dataframe and add column names \"Drill Depth (m)\", \"Drill Pressure (kN)\",\n",
    "        # \"Torque\", \"Flushing Pressure (kN)\".\n",
    "        snd_table_df = df.iloc[:, :4]\n",
    "        snd_table_df.columns = [\"Drill Depth (m)\", \"Drill Pressure (kN)\", \"Torque\", \"Flushing Pressure (kN)\"]\n",
    "\n",
    "        cleaned_table = snd_table_df\n",
    "        \n",
    "        # Extract first two values and transpose arrangement to two different columns \n",
    "        position = snd_table[:2].T\n",
    "        # Name columns \"X-coordinate\" and \"Y-coordinate\"\n",
    "        position.columns = [\"X-coordinate\", \"Y-coordinate\"]\n",
    "        # Populate position column values with as many values in snd_table_df\n",
    "        position = position.append([position]*(len(cleaned_table)-1), ignore_index=True)\n",
    "\n",
    "        #Extracting height above sea level value and deducting value per drill step\n",
    "        height = float(snd_table['v'][2])\n",
    "        height_box = []\n",
    "        drill_step = round(float(df[0][1]) - float(df[0][0]),4)\n",
    "        for depth in range(len(cleaned_table)):\n",
    "            height -= drill_step\n",
    "            height_box.append(round(height,3))\n",
    "            \n",
    "        #Create dataframe for changing depth\n",
    "        height_df = pd.DataFrame(height_box, columns = ['Height Above Sea Level'])\n",
    "        \n",
    "        #Merging position, height_df and cleaned_table dataframes \n",
    "        snd_df = pd.concat([position, height_df, cleaned_table], axis = 1)\n",
    "\n",
    "        #Converting all columns in merged dataframe to numeric data type \n",
    "        for i in range(0, len(snd_df.columns)):\n",
    "            snd_df.iloc[:,i] = pd.to_numeric(snd_df.iloc[:,i], errors='ignore')\n",
    "        #Removing rows with empty Torque values \n",
    "        snd_df = snd_df[snd_df.Torque.notnull()]\n",
    "        #Converting Drill Depth column to float \n",
    "        snd_df[\"Drill Depth (m)\"] = snd_df[\"Drill Depth (m)\"].astype(float)\n",
    "        \n",
    "        #Read workbook in datapairlist\n",
    "        book = openpyxl.load_workbook(pair[0], data_only = True)\n",
    "        sheetname_list = []\n",
    "        #Specifying search pattern for only spreadsheets named with integer numbers\n",
    "        pattern = '^[0-9]*$'\n",
    "        #Search through sheets and only add spreadsheet names named with integer numbers\n",
    "        for sheetname in book.sheetnames:\n",
    "            if re.match(pattern, sheetname):\n",
    "                sheetname_list.append(sheetname)\n",
    "        workbook_depth_list = []        \n",
    "        workbook_material_list = []\n",
    "        # Go through spreadsheet and extract soil depth tested for\n",
    "        for sheet in sheetname_list:\n",
    "            sheet_depth_list = []\n",
    "            for value in book[sheet].iter_rows(min_row=8,\n",
    "                                      max_row=24,\n",
    "                                      min_col=4,\n",
    "                                      max_col=4,\n",
    "                                      values_only=True):\n",
    "                if value[0] is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    sheet_depth_list.append(value[0]) \n",
    "            #If values exist in selected spreadsheet, find the min and add drill step value till max is reached \n",
    "            #This captures the values in-between the max and min in the sheet\n",
    "            try:\n",
    "                min_depth_value = min(sheet_depth_list)\n",
    "                max_depth_value = max(sheet_depth_list)\n",
    "\n",
    "                sheet_depth_range_list = [min_depth_value]\n",
    "\n",
    "                while max_depth_value > round(min_depth_value,3):\n",
    "                    min_depth_value += drill_step\n",
    "                    sheet_depth_range_list.append(round(min_depth_value,3))\n",
    "                workbook_depth_list.append(sheet_depth_range_list)\n",
    "                \n",
    "            #Extract soil type for spreadsheet and duplicate values for length of spreadsheet depth\n",
    "                for val in book[sheet].iter_rows(min_row=24,\n",
    "                                          max_row=24,\n",
    "                                          min_col=15,\n",
    "                                          max_col=15,\n",
    "                                          values_only = True):\n",
    "                    soil_list = [soil_type[val[0]] for material in range(len(sheet_depth_range_list))]\n",
    "                workbook_material_list.append(soil_list)\n",
    "            except:\n",
    "                continue\n",
    "            #Flattening nested lists to singular list \n",
    "            workbook_depth_list = flattenNestedList(workbook_depth_list)\n",
    "            workbook_material_list = flattenNestedList(workbook_material_list)\n",
    "            \n",
    "            workbook_df = pd.DataFrame(list(zip(workbook_depth_list, workbook_material_list)),\n",
    "                                       columns =['Drill Depth (m)', 'Soil type']) \n",
    "            \n",
    "            #Converting all workbook columns to numeric type\n",
    "            for i in range(0, len(workbook_df.columns)):\n",
    "                workbook_df.iloc[:,i] = pd.to_numeric(workbook_df.iloc[:,i], errors='ignore')\n",
    "                \n",
    "        #Merging SND and Workbook Dataframes into one\n",
    "        merged_df = pd.merge(left=workbook_df, right=snd_df, how='left', left_on='Drill Depth (m)',\n",
    "                             right_on='Drill Depth (m)')\n",
    "        \n",
    "        #Adding all merged dataframes to one Dataframe\n",
    "        big_merge_df = big_merge_df.append(merged_df, ignore_index = True, sort = False)\n",
    "\n",
    "    except:\n",
    "        #Adding all incompatible pairs to incompatible_pairs list\n",
    "        incompatible_pairs.append(pair)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting big_merge_df to csv file to be saved locally at the same file location\n",
    "big_merge_df.to_csv('data_extract.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building and Feature Engineering \n",
    "# Reading exported dataset \n",
    "dataset = pd.read_csv(r\"data_extract.csv\", index_col = 0)\n",
    "# Dropping rows containing emoty cells \n",
    "dataset = dataset.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n",
    "\n",
    "# Splitting dataset into independent and dependent sets where soil type is the dependent set\n",
    "y = dataset[\"Soil type\"]\n",
    "X = dataset.drop(\"Soil type\", axis = 1)\n",
    "\n",
    "#Splitting the sets into train and test sets with test set size set to 30% of the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 50) \n",
    "\n",
    "#Using LightGBM model to learn from the train set.\n",
    "model = LGBMClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Testing some hyperparameters for Lightgbm model to improve model performance \n",
    "gridParams = {\n",
    "    'learning_rate': [0.1, 0.05],\n",
    "    'n_estimators': [40, 200, 400],\n",
    "    'num_leaves': [20, 40],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['multiclass'],\n",
    "    'random_state' : [42], \n",
    "    'colsample_bytree' : [0.8, 1],\n",
    "    'subsample' : [0.75,1],\n",
    "    'reg_alpha' : [1,0.5],\n",
    "    'reg_lambda' : [1,0.5],\n",
    "    }\n",
    "#Using gridsearch to test through specified model hyperparameters and determine best values\n",
    "grid = GridSearchCV(model, gridParams,\n",
    "                    verbose=0,\n",
    "                    cv=4,\n",
    "                    n_jobs=2)\n",
    "#fitting gridsearch on train data\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying model hyperparameters from gridsearch \n",
    "gbm = LGBMClassifier(boosting_type = 'gbdt', colsample_bytree = 0.8, learning_rate = 0.1, \n",
    "                     n_estimators = 400, num_leaves = 20, objective = 'multiclass', \n",
    "                     random_state = 42, reg_alpha = 0.5, reg_lambda = 0.5, subsample = 0.75)\n",
    "#Refitting model \n",
    "gbm.fit(X_train, y_train)\n",
    "#Using model to predict test dependent variable\n",
    "Y_sum = gbm.predict(X_test)\n",
    "\n",
    "#Model Accuracy\n",
    "accuracy = metrics.accuracy_score(y_test, Y_sum)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "#Model Precision\n",
    "precision = metrics.precision_score(y_test, Y_sum, average = 'macro')\n",
    "print('Precision: %f' % precision)\n",
    "#Model Recall\n",
    "recall = metrics.recall_score(y_test, Y_sum, average = 'macro')\n",
    "print('Recall: %f' % recall)\n",
    "#Model F1 score \n",
    "f1 = metrics.f1_score(y_test, Y_sum, average = 'macro')\n",
    "print('F1 score: %f' % f1)\n",
    "\n",
    "print(\"\")\n",
    "#Model Cross validation score \n",
    "print('Cross-Validation Score: %f' % np.mean(cross_val_score(gbm, X, y, cv=10)))\n",
    "\n",
    "#Stop program time \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
